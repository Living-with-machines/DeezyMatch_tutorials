{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: HGL example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from DeezyMatch import train as dm_train\n",
    "\n",
    "# train a new model\n",
    "dm_train(input_file_path=\"inputs/input_dfm.yaml\", \n",
    "         dataset_path=\"data/libyan_pairs.txt\", \n",
    "         model_name=\"hgl001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DeezyMatch import plot_log\n",
    "\n",
    "# plot log file\n",
    "plot_log(path2log=\"./models/hgl001/log.txt\", \n",
    "         output_name=\"log_hgl001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from DeezyMatch import inference as dm_inference\n",
    "\n",
    "# model inference using a model stored at pretrained_model_path and pretrained_vocab_path \n",
    "dm_inference(input_file_path=\"inputs/input_dfm.yaml\",\n",
    "             dataset_path=\"data/libyan_pairs.txt\", \n",
    "             pretrained_model_path=\"./models/hgl001/hgl001.model\", \n",
    "             pretrained_vocab_path=\"./models/hgl001/hgl001.vocab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate query vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DeezyMatch import inference as dm_inference\n",
    "\n",
    "# generate vectors for queries (specified in dataset_path) \n",
    "# using a model stored at pretrained_model_path and pretrained_vocab_path \n",
    "dm_inference(input_file_path=\"inputs/input_dfm.yaml\",\n",
    "            dataset_path=\"data/queries.txt\", \n",
    "            pretrained_model_path=\"./models/hgl001/hgl001.model\", \n",
    "            pretrained_vocab_path=\"./models/hgl001/hgl001.vocab\",\n",
    "            inference_mode=\"vect\",\n",
    "            scenario=\"queries/test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate candidate vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DeezyMatch import inference as dm_inference\n",
    "\n",
    "# generate vectors for candidates (specified in dataset_path) \n",
    "# using a model stored at pretrained_model_path and pretrained_vocab_path \n",
    "dm_inference(input_file_path=\"inputs/input_dfm.yaml\",\n",
    "             dataset_path=\"data/candidates.txt\", \n",
    "             pretrained_model_path=\"./models/hgl001/hgl001.model\", \n",
    "             pretrained_vocab_path=\"./models/hgl001/hgl001.vocab\",\n",
    "             inference_mode=\"vect\",\n",
    "             scenario=\"candidates/test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assembling queries vector representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from DeezyMatch import combine_vecs\n",
    "\n",
    "# combine vectors stored in queries/test and save them in combined/queries_test\n",
    "combine_vecs(rnn_passes=['fwd', 'bwd'], \n",
    "             input_scenario='queries/test', \n",
    "             output_scenario='combined/queries_test', \n",
    "             print_every=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assembling candidates vector representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from DeezyMatch import combine_vecs\n",
    "\n",
    "# combine vectors stored in candidates/test and save them in combined/candidates_test\n",
    "combine_vecs(rnn_passes=['fwd', 'bwd'], \n",
    "             input_scenario='candidates/test', \n",
    "             output_scenario='combined/candidates_test', \n",
    "             print_every=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Candidate Ranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from DeezyMatch import candidate_ranker\n",
    "\n",
    "# Select candidates based on L2-norm distance (aka faiss distance):\n",
    "# find candidates from candidate_scenario \n",
    "# for queries specified in query_scenario\n",
    "candidates_pd = \\\n",
    "    candidate_ranker(query_scenario=\"./combined/queries_test\",\n",
    "                     candidate_scenario=\"./combined/candidates_test\", \n",
    "                     ranking_metric=\"faiss\", \n",
    "                     selection_threshold=15., \n",
    "                     num_candidates=3, \n",
    "                     search_size=3, \n",
    "                     verbose=False,\n",
    "                     use_predict=False,\n",
    "                     output_path=\"ranker_results/test_candidates_deezymatch\", \n",
    "                     pretrained_model_path=\"./models/hgl001/hgl001.model\", \n",
    "                     pretrained_vocab_path=\"./models/hgl001/hgl001.vocab\", \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "candidates_pd.iloc[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "26a5527aa58fafc67e53d83503597f8ba110345d20ac6660bedc50bf2ce973f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
