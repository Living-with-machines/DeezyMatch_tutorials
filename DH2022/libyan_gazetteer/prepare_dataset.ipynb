{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import string\n",
    "import random\n",
    "import pandas as pd\n",
    "from thefuzz import fuzz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Heritage Gazetteer of Libya"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Intro by Valeria]\n",
    "\n",
    "Source: https://slsgazetteer.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "In order to train and use DeezyMatch to find candidates for toponyms in the HGL, we need to prepare the following datasets:\n",
    "* **String matching dataset:** dataset of toponym pairs built from Geonames alternate names belonging to places in current-day Libya.\n",
    "* **Candidates dataset:** list of toponyms (and alternate names) belonging to places in current-day Libya, from Geonames.\n",
    "* **Queries dataset:** list of toponyms obtained from the Heritage Gazetteer of Libya.\n",
    "\n",
    "We provide the [vocabulary file](https://github.com/Living-with-machines/DeezyMatch/examples/libyan_gazetteer/inputs/characters_v001.vocab) and the [input file](https://github.com/Living-with-machines/DeezyMatch/examples/libyan_gazetteer/inputs/input_dfm.yaml)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining the data from Geonames\n",
    "\n",
    "We focus on **toponyms of places in modern-day Libya** in this case study, so we will just download `LY`, but you can use any other country (see a mapping between countries and codes here: https://www.geonames.org/countries/).\n",
    "\n",
    "Download the following data from [Geonames](https://download.geonames.org/export/dump/), in particular:\n",
    "* Download `LY.zip`: https://download.geonames.org/export/dump/LY.zip (depends on the country you're interested in)\n",
    "* Download `alternateNamesV2.zip`: https://download.geonames.org/export/dump/alternateNamesV2.zip (for all countries)\n",
    "\n",
    "Unzip the files and store `LY.txt` and `alternateNamesV2.txt` in `data/`.\n",
    "\n",
    "### Obtaining the data from the HGL\n",
    "\n",
    "The Historical Gazetteer of Lybia data can be found as a json [here](http://slsgazetteer.org/data/downloads/json/dump.json). Download it and store it in `data/`, renaming it as `hgl_data.json`.\n",
    "\n",
    "### Directory structure\n",
    "\n",
    "The `libyan_gazetteer` directory should now look like this:\n",
    "```\n",
    "libyan_gazetteer\n",
    "   ├── prepare_dataset.ipynb\n",
    "   ├── tutorial_hgl.ipynb\n",
    "   ├── data\n",
    "   │   ├── hgl_data.json\n",
    "   │   ├── LY.txt\n",
    "   │   └── alternateNamesV2.txt\n",
    "   └── inputs\n",
    "       ├── characters_v001.vocab\n",
    "       └── input_dfm.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the string pairs dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the country of choice:\n",
    "country = \"LY\"\n",
    "\n",
    "# Specify the language codes of the country, for this example: Arabic, Libyan Arabic, Berber, Domari,\n",
    "# Tamasheq, Teda, Egyptian Spoken Arabic, Standard Arabic, Awjila, Italian, French, English, and Libyan\n",
    "# Spoken Arabic:\n",
    "toponym_languages = [\"ar\", \"ar-LY\", \"ber\", \"rmt\", \"taq\", \"tuq\", \"arz\", \"arb\", \"auj\", \"it\", \"fr\", \"en\", \"ayl\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the country-specific gazetteer (we need to specify the column names):\n",
    "df_country = pd.read_csv(\"data/\" + country + \".txt\", sep=\"\\t\", names=[\n",
    "                                                        \"geonameid\", \n",
    "                                                        \"name\", \n",
    "                                                        \"asciiname\", \n",
    "                                                        \"alternatenames\", \n",
    "                                                        \"latitude\", \n",
    "                                                        \"longitude\", \n",
    "                                                        \"feature class\", \n",
    "                                                        \"feature code\", \n",
    "                                                        \"country code\", \n",
    "                                                        \"cc2\", \n",
    "                                                        \"admin1 code\", \n",
    "                                                        \"admin2 code\", \n",
    "                                                        \"admin3 code\", \n",
    "                                                        \"admin4 code\", \n",
    "                                                        \"population\", \n",
    "                                                        \"elevation\", \n",
    "                                                        \"dem\", \n",
    "                                                        \"timezone\", \n",
    "                                                        \"modification date\"\n",
    "                                                        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the place classes we're interested in:\n",
    "# For reference:\n",
    "# * A: country, state, region,...\n",
    "# * H: stream, lake, ...\n",
    "# * L: parks,area, ...\n",
    "# * P: city, village,...\n",
    "# * R: road, railroad \n",
    "# * S: spot, building, farm\n",
    "# * T: mountain,hill,rock,... \n",
    "# * U: undersea\n",
    "# * V: forest,heath,...\n",
    "fclasses = [\"A\", \"H\", \"L\", \"P\", \"R\", \"T\", \"V\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter locations by their feature class:\n",
    "df_country = df_country[df_country[\"feature class\"].isin(fclasses)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns:\n",
    "df_country = df_country.drop(columns=[\"feature code\", \"country code\", \"cc2\", \"admin1 code\",\n",
    "                                      \"admin2 code\", \"admin3 code\", \"admin4 code\", \"population\", \n",
    "                                      \"elevation\", \"dem\", \"timezone\", \"modification date\",\n",
    "                                      \"feature class\", \"alternatenames\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the alternate names file:\n",
    "altnames_df = pd.read_csv(\"data/alternateNamesV2.txt\",\n",
    "                          sep=\"\\t\", \n",
    "                          low_memory=False, \n",
    "                          names=[\n",
    "                                \"alternateNameId\", \n",
    "                                \"geonameid\", \n",
    "                                \"isolanguage\", \n",
    "                                \"alternateName\", \n",
    "                                \"isPreferredName\", \n",
    "                                \"isShortName\", \n",
    "                                \"isColloquial\", \n",
    "                                \"isHistoric\", \n",
    "                                \"from\", \n",
    "                                \"to\"\n",
    "                                ],\n",
    "                            usecols=[\n",
    "                                \"geonameid\", \n",
    "                                \"isolanguage\",\n",
    "                                \"alternateName\"\n",
    "                            ]\n",
    "                        )\n",
    "\n",
    "# Filter the alternate names to keep those in the languages we are interested in:\n",
    "altnames_df = altnames_df.loc[altnames_df[\"isolanguage\"].isin(toponym_languages)]\n",
    "\n",
    "# Aggregate alternate names into a list, aggregated by geonames id:\n",
    "altnames_df = altnames_df.groupby(['geonameid'], as_index=False).agg({'alternateName': lambda x: x.tolist()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the first rows of the `altnames_df` dataframe:\n",
    "altnames_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the country dataframe with the altnames dataframe by geonames id,\n",
    "# using only keys from the country dataframe (therefore, dropping alternate\n",
    "# names that are not in the country of interest):\n",
    "dataset_df = pd.merge(df_country, altnames_df, on=\"geonameid\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolidate the different name and altname columns into one list per geonames id:\n",
    "altnames = []\n",
    "for i, row in dataset_df.iterrows():\n",
    "    current_altnames = []\n",
    "    current_altnames.append(row[\"name\"])\n",
    "    current_altnames.append(row[\"asciiname\"])\n",
    "    if type(row[\"alternateName\"]) == list:\n",
    "        current_altnames += row[\"alternateName\"]\n",
    "    altnames.append(list(set(current_altnames)))\n",
    "dataset_df = dataset_df.drop(columns = [\"name\", \"asciiname\", \"alternateName\"])\n",
    "dataset_df[\"altnames\"] = altnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the first rows of the `dataset_df` dataframe:\n",
    "dataset_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary that maps the geonames id of a location to the list of alternate names:\n",
    "location_to_toponyms = dict()\n",
    "for i, row in dataset_df.iterrows():\n",
    "    location = row[\"geonameid\"]\n",
    "    altnames = row[\"altnames\"]\n",
    "    location_to_toponyms[location] = altnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary that maps a toponym variation to the list of possible geonames id:\n",
    "toponym_to_locations = dict()\n",
    "for location in location_to_toponyms:\n",
    "    for toponym in location_to_toponyms[location]:\n",
    "        if toponym in toponym_to_locations:\n",
    "            toponym_to_locations[toponym].append(location)\n",
    "        else:\n",
    "            toponym_to_locations[toponym] = [location]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all toponyms in the country:\n",
    "all_toponyms = []\n",
    "for k in location_to_toponyms:\n",
    "    if type(location_to_toponyms[k]) == list:\n",
    "        all_toponyms += location_to_toponyms[k]\n",
    "all_toponyms = list(set(all_toponyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils: map punctuation to white spaces, for token-based Jaccard similarity needed below:\n",
    "punctuation = string.punctuation + \"’\"\n",
    "translator = str.maketrans(string.punctuation, ' '*len(string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get positive matches from Geonames pairs of toponyms, if\n",
    "# their similarity is > 0.60 or if Jaccard similarity of its\n",
    "# tokens is larger than 0.5:\n",
    "positive_matches = []\n",
    "for k in location_to_toponyms:\n",
    "    if type(location_to_toponyms[k]) == list:\n",
    "        for toponym1 in location_to_toponyms[k]:\n",
    "            for toponym2 in location_to_toponyms[k]:\n",
    "                # Character-based string similarity:\n",
    "                if fuzz.ratio(toponym1, toponym2) > 60:\n",
    "                    positive_matches.append(toponym1 + \"\\t\" + toponym2 + \"\\t\" + \"TRUE\")\n",
    "                # Token-based string similarity:\n",
    "                else:\n",
    "                    s1 = set(toponym1.translate(translator).split(\" \"))\n",
    "                    s2 = set(toponym2.translate(translator).split(\" \"))\n",
    "                    if float(len(s1.intersection(s2)) / len(s1.union(s2))) >= 0.5:\n",
    "                        positive_matches.append(toponym1 + \"\\t\" + toponym2 + \"\\t\" + \"TRUE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get negative matches (the same number as positive matches)\n",
    "# from Geonames pairs of toponyms, if their string similarity\n",
    "# is < 0.40 or if Jaccard similarity of its tokens is less than 0.2:\n",
    "negative_matches = []\n",
    "while len(negative_matches) < len(positive_matches):\n",
    "    random_pair = random.choices(all_toponyms, k=2)\n",
    "    toponym1 = random_pair[0]\n",
    "    toponym2 = random_pair[1]\n",
    "    # Character-based string similarity:\n",
    "    if fuzz.ratio(toponym1, toponym2) < 40:\n",
    "        negative_matches.append(toponym1 + \"\\t\" + toponym2 + \"\\t\" + \"FALSE\")\n",
    "    # Token-based string similarity:\n",
    "    else:\n",
    "        s1 = set(toponym1.translate(translator).split(\" \"))\n",
    "        s2 = set(toponym2.translate(translator).split(\" \"))\n",
    "        if float(len(s1.intersection(s2)) / len(s1.union(s2))) < 0.2:\n",
    "            negative_matches.append(toponym1 + \"\\t\" + toponym2 + \"\\t\" + \"FALSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write string pairs into a file (this is the string matching dataset):\n",
    "with open(\"data/libyan_pairs.txt\", \"w\") as fw:\n",
    "    for nm in negative_matches:\n",
    "        fw.write(nm + \"\\n\")\n",
    "    for pm in positive_matches:\n",
    "        fw.write(pm + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the candidates dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The candidates dataset is created from all toponyms and variations from\n",
    "# the country of interest:\n",
    "candidates = list(toponym_to_locations.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the candidates dataset, with one toponym per line\n",
    "with open(\"data/candidates.txt\", \"w\") as fw:\n",
    "    for c in set(candidates):\n",
    "        fw.write(c + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the queries dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the HGL data json file:\n",
    "with open('data/hgl_data.json') as json_file:\n",
    "    data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the queries dataset, with one toponym per line\n",
    "with open(\"data/queries.txt\", \"w\") as fw:\n",
    "    for entry in data[\"features\"]:\n",
    "        fw.write(entry[\"title\"].split(\",\")[0] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py39deezy)",
   "language": "python",
   "name": "py39deezy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "61b4062b24dfb1010f420dad5aa3bd73a4d2af47d0ec44eafec465a35a9d7239"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
